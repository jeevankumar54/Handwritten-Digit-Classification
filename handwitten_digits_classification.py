# -*- coding: utf-8 -*-
"""Handwitten_Digits_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uBGzOKsCyqnLETDCdnBCUu4m8PbyDUu0
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np

(X_train, y_train) , (X_test, y_test) = keras.datasets.mnist.load_data()

len(X_train)

len(X_test)

X_train[0]

plt.matshow(X_train[2])

y_train[2]

X_train.shape

#Convert 2d array into 1d array. (Flatteing)
X_train_flattened = X_train.reshape(len(X_train), 28*28)
X_test_flattened = X_test.reshape(len(X_test), 28*28)

X_train_flattened[0]

model = keras.Sequential([
    keras.layers.Dense(10, input_shape = (784,), activation = 'sigmoid')
])

model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

model.fit(X_train_flattened, y_train, epochs = 5)

"""Lets scale the values in 2d array to get more accuracy. Divide every value by 255 to get in the range of 0 to 1."""

X_train = X_train/255
X_test = X_test/255

X_train[0]

X_train_flattened = X_train.reshape(len(X_train), 28*28)
X_test_flattened = X_test.reshape(len(X_test), 28*28)

model.fit(X_train_flattened, y_train, epochs = 5)

model.evaluate(X_test_flattened, y_test)

#Predicting all the values in test set
y_predicted = model.predict(X_test_flattened)

plt.matshow(X_test[3])

y_predicted[3]  #it is storing the scores of all 10 digits, we need to detect the largest among all

# find the max value and print its index
np.argmax(y_predicted[3])

y_predicted_labels = [np.argmax(i) for i in y_predicted]

cm = tf.math.confusion_matrix(labels= y_test, predictions = y_predicted_labels)
cm

# Visualising the above confusion matrix in better way
import seaborn as sn
plt.figure(figsize = (10, 7))
sn.heatmap(cm, annot = True, fmt = 'd')
plt.xlabel('Predictions')
plt.ylabel('Truth')

"""Adding Hidden layer"""

model = keras.Sequential([
    keras.layers.Dense(100, input_shape = (784,), activation = 'relu'),
    keras.layers.Dense(10, activation = 'sigmoid')
])

model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

model.fit(X_train_flattened, y_train, epochs = 5)

"""  We can observe that by adding the hidden layer, accuracy increased"""

model.evaluate(X_test_flattened, y_test)

cm = tf.math.confusion_matrix(labels= y_test, predictions = y_predicted_labels)

# Visualising the above confusion matrix in better way
import seaborn as sn
plt.figure(figsize = (10, 7))
sn.heatmap(cm, annot = True, fmt = 'd')
plt.xlabel('Predictions')
plt.ylabel('Truth')

"""We can also flatten the array using the keras layer called flatten"""

model = keras.Sequential([
    keras.layers.Flatten(input_shape = (28, 28)),
    keras.layers.Dense(100, input_shape = (784,), activation = 'relu'),
    keras.layers.Dense(10, activation = 'sigmoid')
])

model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

model.fit(X_train, y_train, epochs = 5)

